{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data\n",
    "\n",
    "The dataset consists of detection *events*, wherein a variable number of particles come into contact with the detector's cells. The location of these impacts are called *hits*, and there are likewise a variable number of them per particle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100 samples.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "with open('tracks.pickle', 'rb') as f:\n",
    "    samples = pickle.load(f)\n",
    "\n",
    "print(\"Loaded {} samples.\".format(len(samples)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features\n",
    "\n",
    "Each sample contains a set of hits, and each hit contains the following information:\n",
    "\n",
    "* *x,y,z* coordinates\n",
    "* Cell count and impact magnitude\n",
    "* A learned hit embedding, output from the previous graph creation stage\n",
    "* Ground truth cluster ID, denoting the particle which created the hit\n",
    "\n",
    "Additionally, samples contain graphs as output from the previous stage which aims to connect hits created by the same particle. The two graphs included are\n",
    "\n",
    "* A predicted graph, the raw output from the graph building stage\n",
    "* An augmented graph, which contains the predicted graph, plus any connections missed between hits created by the same particle. This is used in the GNN's loss function.\n",
    "\n",
    "### Visualizations\n",
    "\n",
    "Choosing a sample to explore, one can see how the embedding differs from the raw features for graph creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_clusters(x,y,pid):\n",
    "    for g in np.unique(pid):\n",
    "        i = np.where(pid == g)\n",
    "        plt.scatter(x[i],y[i], label=g)\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "hits = samples[0]['hits']\n",
    "xyz = hits['xyz']\n",
    "emb = hits['emb']\n",
    "pid = hits['particle_id']\n",
    "\n",
    "# Hit coordinates\n",
    "plot_clusters(xyz[:,0], xyz[:,1], pid)\n",
    "\n",
    "# Emb coordinates\n",
    "plot_clusters(emb[:,0], emb[:,1], pid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, the embedding will lead to superior clustering as compared with the raw *x,y,z* positions. However, this embedding incorporates information from only each hit individually. With a GNN, one can create node embeddings which incorporate information from the hit's neighborhood. As we will see, this allows for superior embeddings and thus improved performance in clustering.\n",
    "\n",
    "# Model\n",
    "\n",
    "The GNN model chosen is a simple message-passing architecture. One layer concatenates each node's features with an aggregation of the node's neighborhood, before applying a transformation via a fully-connected neural network layer.\n",
    "\n",
    "The output of the model is a set of node embeddings, where this new embedding has the same goal as in the graph building stage: according to some distance metric, node pairs whose hits belong to the same particle should be close, and otherwise they should be far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class GNN(nn.Module):\n",
    "  def __init__(self,\n",
    "               nb_hidden_gnn,\n",
    "               nb_layer,\n",
    "               nb_hidden_kernel,\n",
    "               nb_kernel,\n",
    "               input_dim,\n",
    "               emb_dim=2):\n",
    "    super(GNN, self).__init__()\n",
    "\n",
    "    # Construct first layer\n",
    "    gnn_layers = [GNN_Layer(input_dim,\n",
    "                            nb_hidden_gnn,\n",
    "                            nb_kernel,\n",
    "                            nb_hidden_kernel,\n",
    "                            apply_norm=True,\n",
    "                            softmax=False)]\n",
    "    \n",
    "    # Construct additional layers\n",
    "    for _ in range(nb_layer-1):\n",
    "        gnn_layers.append(GNN_Layer(nb_hidden_gnn,\n",
    "                                    nb_hidden_gnn,\n",
    "                                    nb_kernel,\n",
    "                                    nb_hidden_kernel))\n",
    "    self.layers = nn.ModuleList(gnn_layers)\n",
    "\n",
    "    self.final_emb = nn.Linear(nb_hidden_gnn, emb_dim)\n",
    "\n",
    "  def forward(self, g):\n",
    "    if torch.cuda.is_available():\n",
    "        g.ndata['feat'] = g.ndata.pop('feat').to('cuda', non_blocking=True)\n",
    "\n",
    "    emb = g.ndata.pop('feat')\n",
    "    for i, layer in enumerate(self.layers):\n",
    "      emb = layer(g, emb)\n",
    "    emb = self.final_emb(emb)\n",
    "    return emb\n",
    "\n",
    "def weighted_msg(e):\n",
    "    return {'msg': e.src['feat'] * e.data['e']}\n",
    "\n",
    "\n",
    "class GNN_Layer(nn.Module):\n",
    "    def __init__(self,\n",
    "               input_dim,\n",
    "               nb_hidden_gnn,\n",
    "               nb_kernel,\n",
    "               nb_hidden_kernel,\n",
    "               apply_norm=True,\n",
    "               softmax=False):\n",
    "        super(GNN_Layer, self).__init__()\n",
    "\n",
    "        if softmax:\n",
    "            self.kernel = MLP_Kernel_DGL_Softmax(input_dim, nb_hidden_kernel)\n",
    "        else:\n",
    "            self.kernel = MLP_Kernel_DGL(input_dim, nb_hidden_kernel)\n",
    "\n",
    "        self.gconv = DGL_Convolution(input_dim, nb_hidden_gnn)\n",
    "        self.bn = nn.BatchNorm1d(input_dim,momentum=0.10) if apply_norm else None\n",
    "\n",
    "    def forward(self, g, features):\n",
    "        # maybe apply normalization\n",
    "        if self.bn is not None:\n",
    "            features = self.bn(features)\n",
    "        g.ndata['feat'] = features\n",
    "\n",
    "        # set edge weights for this layer\n",
    "        g = self.kernel(g)\n",
    "        \n",
    "        # send weighted messages and apply graph convolution to nodes\n",
    "        g.send_and_recv(g.edges(),\n",
    "                        message_func=weighted_msg,\n",
    "                        reduce_func=dgl.function.sum(msg='msg', out='agg_msg'),\n",
    "                        apply_node_func=self.gconv)\n",
    "        g.ndata.pop('feat')\n",
    "        g.ndata.pop('agg_msg')\n",
    "        return g.ndata.pop('emb')\n",
    "\n",
    "class DGL_Convolution(nn.Module):\n",
    "    def __init__(self,\n",
    "               input_dim,\n",
    "               nb_hidden_gnn):\n",
    "        super(DGL_Convolution, self).__init__()\n",
    "        self.weights = nn.Linear(2*input_dim, nb_hidden_gnn)\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, n):\n",
    "        feats = n.data['feat']\n",
    "        agg_msg = n.data['agg_msg']\n",
    "        node_feats = torch.cat((feats, agg_msg), dim=1)\n",
    "        emb = self.weights(node_feats)\n",
    "        emb = self.act(emb)\n",
    "        return {'emb':emb}\n",
    "\n",
    "\n",
    "class MLP_Kernel_DGL(nn.Module):\n",
    "    def __init__(self, nb_input, nb_hidden_gnn, nb_output=1, nb_layer=1):\n",
    "        super(MLP_Kernel_DGL, self).__init__()\n",
    "        layers = [nn.Linear(nb_input*2, nb_hidden_gnn)]\n",
    "        for _ in range(nb_layer-1):\n",
    "            layers.append(nn.Linear(nb_hidden_gnn, nb_hidden_gnn))\n",
    "        layers.append(nn.Linear(nb_hidden_gnn, nb_output))\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.act2 = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, g):\n",
    "        g.apply_edges(self.mlp)\n",
    "        return g\n",
    "\n",
    "    def mlp(self, e):\n",
    "        # Gather features from all relevant node pairs\n",
    "        src = e.src['feat']\n",
    "        dst = e.dst['feat']\n",
    "        e_feats = torch.cat((src,dst),dim=1)\n",
    "        \n",
    "        # Apply MLP layers to node pairs\n",
    "        for l in self.layers[:-1]:\n",
    "            e_feats = self.act1(l(e_feats))\n",
    "        \n",
    "        # Apply final output with sigmoid\n",
    "        e_feats = self.layers[-1](e_feats)\n",
    "        e_feats = self.act2(e_feats)\n",
    "        return {'e':e_feats}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset, Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "def get_edge_indices(edges):\n",
    "    edge_pairs = []\n",
    "    for i, neighbors in enumerate(edges):\n",
    "        for e_idx in neighbors:\n",
    "            edge_pairs.append([i,e_idx])\n",
    "    return edge_pairs\n",
    "\n",
    "def get_true_edge_values(pred_edge_idx, true_edges):\n",
    "    values = [0] * len(pred_edge_idx)\n",
    "    for i, (src, dst) in enumerate(pred_edge_idx):\n",
    "        if dst in true_edges[src]:\n",
    "            values[i] = 1\n",
    "    return values\n",
    "\n",
    "class TrackML_Dataset(Dataset):\n",
    "    def __init__(self, samples):\n",
    "        self.samples = samples\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        s = self.samples[index]\n",
    "        \n",
    "        hits = s['hits']\n",
    "        xyz  = hits['xyz']\n",
    "        emb  = hits['emb']\n",
    "        hits = torch.FloatTensor(np.concatenate((xyz, emb), axis=1))\n",
    "\n",
    "        graphs = s['graphs']\n",
    "        pred_edges = graphs['pred']\n",
    "        loss_edges = graphs['loss']\n",
    "        true_edges = graphs['true']\n",
    "    \n",
    "        pred_edge_idx = get_edge_indices(pred_edges)\n",
    "        true_edge_idx = get_edge_indices(loss_edges)\n",
    "        true_edge_values = get_true_edge_values(true_edge_idx,true_edges)\n",
    "\n",
    "        # Build inference graph\n",
    "        g_input = dgl.DGLGraph()\n",
    "        g_input.add_nodes(len(hits))\n",
    "        src, dst = tuple(zip(*pred_edge_idx))\n",
    "        g_input.add_edges(src, dst)\n",
    "        g_input.ndata['feat'] = hits\n",
    "\n",
    "        # Build ground truth graph\n",
    "        g_true = dgl.DGLGraph()\n",
    "        g_true.add_nodes(len(hits))\n",
    "        src, dst = tuple(zip(*true_edge_idx))\n",
    "        g_true.add_edges(src, dst)\n",
    "        g_true.edata['truth'] = torch.FloatTensor(true_edge_values)\n",
    "        \n",
    "        g_input.set_n_initializer(dgl.init.zero_initializer)\n",
    "        g_true.set_n_initializer(dgl.init.zero_initializer)\n",
    "        g_input.set_e_initializer(dgl.init.zero_initializer)\n",
    "        g_true.set_e_initializer(dgl.init.zero_initializer)\n",
    "        \n",
    "        return g_input, g_true\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "def trackml_collate(sample):\n",
    "    g_input = [s[0] for s in sample]\n",
    "    g_input = dgl.batch(g_input)\n",
    "\n",
    "    g_true = [s[1] for s in sample]\n",
    "    g_true = dgl.batch(g_true)\n",
    "\n",
    "    return g_input, g_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNN(\n",
      "  (layers): ModuleList(\n",
      "    (0): GNN_Layer(\n",
      "      (kernel): MLP_Kernel_DGL(\n",
      "        (layers): ModuleList(\n",
      "          (0): Linear(in_features=12, out_features=16, bias=True)\n",
      "          (1): Linear(in_features=16, out_features=1, bias=True)\n",
      "        )\n",
      "        (act1): ReLU()\n",
      "        (act2): Sigmoid()\n",
      "      )\n",
      "      (gconv): DGL_Convolution(\n",
      "        (weights): Linear(in_features=12, out_features=16, bias=True)\n",
      "        (act): ReLU()\n",
      "      )\n",
      "      (bn): BatchNorm1d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): GNN_Layer(\n",
      "      (kernel): MLP_Kernel_DGL(\n",
      "        (layers): ModuleList(\n",
      "          (0): Linear(in_features=32, out_features=16, bias=True)\n",
      "          (1): Linear(in_features=16, out_features=1, bias=True)\n",
      "        )\n",
      "        (act1): ReLU()\n",
      "        (act2): Sigmoid()\n",
      "      )\n",
      "      (gconv): DGL_Convolution(\n",
      "        (weights): Linear(in_features=32, out_features=16, bias=True)\n",
      "        (act): ReLU()\n",
      "      )\n",
      "      (bn): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): GNN_Layer(\n",
      "      (kernel): MLP_Kernel_DGL(\n",
      "        (layers): ModuleList(\n",
      "          (0): Linear(in_features=32, out_features=16, bias=True)\n",
      "          (1): Linear(in_features=16, out_features=1, bias=True)\n",
      "        )\n",
      "        (act1): ReLU()\n",
      "        (act2): Sigmoid()\n",
      "      )\n",
      "      (gconv): DGL_Convolution(\n",
      "        (weights): Linear(in_features=32, out_features=16, bias=True)\n",
      "        (act): ReLU()\n",
      "      )\n",
      "      (bn): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): GNN_Layer(\n",
      "      (kernel): MLP_Kernel_DGL(\n",
      "        (layers): ModuleList(\n",
      "          (0): Linear(in_features=32, out_features=16, bias=True)\n",
      "          (1): Linear(in_features=16, out_features=1, bias=True)\n",
      "        )\n",
      "        (act1): ReLU()\n",
      "        (act2): Sigmoid()\n",
      "      )\n",
      "      (gconv): DGL_Convolution(\n",
      "        (weights): Linear(in_features=32, out_features=16, bias=True)\n",
      "        (act): ReLU()\n",
      "      )\n",
      "      (bn): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (final_emb): Linear(in_features=16, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# PARAMETERS\n",
    "batch_size = 4\n",
    "nb_hidden = 16\n",
    "nb_layers = 4\n",
    "learn_rate = 0.001\n",
    "\n",
    "dataset = TrackML_Dataset(samples)\n",
    "dataloader = DataLoader(dataset, \n",
    "                        batch_size=batch_size, \n",
    "                        collate_fn=trackml_collate, \n",
    "                        drop_last=True, \n",
    "                        shuffle=True)\n",
    "\n",
    "net = GNN(nb_hidden, nb_layers, nb_hidden, 1, 6)\n",
    "optim = torch.optim.Adamax(net.parameters(), lr=learn_rate)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training on 100 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicholas/venv/kdd/lib64/python3.7/site-packages/dgl/base.py:18: UserWarning: Initializer is not set. Use zero initializer instead. To suppress this warning, use `set_initializer` to explicitly specify which initializer to use.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  48  Loss: 0.288  Acc: 70.5\n",
      "  96  Loss: 0.245  Acc: 75.9\n",
      "\n",
      "Training on 100 samples\n",
      "  48  Loss: 0.161  Acc: 85.1\n",
      "  96  Loss: 0.139  Acc: 87.9\n",
      "\n",
      "Training on 100 samples\n",
      "  48  Loss: 0.111  Acc: 90.9\n",
      "  96  Loss: 0.105  Acc: 91.0\n",
      "\n",
      "Training on 100 samples\n",
      "  48  Loss: 0.096  Acc: 92.5\n",
      "  96  Loss: 0.092  Acc: 92.9\n",
      "\n",
      "Training on 100 samples\n",
      "  48  Loss: 0.085  Acc: 93.0\n",
      "  96  Loss: 0.081  Acc: 93.4\n",
      "\n",
      "Training on 100 samples\n",
      "  48  Loss: 0.077  Acc: 94.2\n",
      "  96  Loss: 0.078  Acc: 94.0\n",
      "\n",
      "Training on 100 samples\n",
      "  48  Loss: 0.066  Acc: 94.7\n",
      "  96  Loss: 0.071  Acc: 94.3\n",
      "\n",
      "Training on 100 samples\n",
      "  48  Loss: 0.071  Acc: 95.0\n",
      "  96  Loss: 0.068  Acc: 95.1\n",
      "\n",
      "Training on 100 samples\n",
      "  48  Loss: 0.061  Acc: 95.6\n",
      "  96  Loss: 0.064  Acc: 95.3\n",
      "\n",
      "Training on 100 samples\n",
      "  48  Loss: 0.058  Acc: 95.9\n",
      "  96  Loss: 0.059  Acc: 95.8\n"
     ]
    }
   ],
   "source": [
    "def get_emb_for_loss(e):\n",
    "    src = e.src['emb']\n",
    "    dst = e.dst['emb']\n",
    "    truth = e.data['truth']\n",
    "    pred_dst = nn.functional.pairwise_distance(src, dst)\n",
    "    true_dst = truth*2 -1\n",
    "    loss = nn.functional.hinge_embedding_loss(pred_dst, true_dst, reduction='none')\n",
    "    return {'loss':loss, 'pred_dst':pred_dst, 'true_dst':true_dst}\n",
    "\n",
    "def score_dist_accuracy(pred, true):\n",
    "    pred = pred.round()\n",
    "    pred[pred!=0] = 1\n",
    "    pred = 1-pred\n",
    "    correct = pred==true\n",
    "    nb_correct = correct.sum()\n",
    "    nb_total = true.size(0)\n",
    "    score = float(nb_correct.item()) / nb_total\n",
    "    return score\n",
    "\n",
    "def train_one_epoch(net, batch_size, optimizer, train_loader):\n",
    "    net.train()\n",
    "\n",
    "    nb_batch = len(train_loader)\n",
    "    nb_train = nb_batch * batch_size\n",
    "    epoch_score = 0\n",
    "    epoch_loss  = 0\n",
    "\n",
    "    print(\"\\nTraining on {} samples\".format(nb_train))\n",
    "    for i, (g_input, g_true) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        f = g_input.ndata['feat']\n",
    "\n",
    "        hits_emb = net(g_input)\n",
    "        g_true.ndata['emb'] = hits_emb\n",
    "        \n",
    "\n",
    "        g_true.apply_edges(get_emb_for_loss)\n",
    "\n",
    "        loss = g_true.edata.pop('loss').mean()\n",
    "        score = score_dist_accuracy(g_true.edata.pop('pred_dst'),\n",
    "                                          g_true.edata.pop('truth'))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_score += score * 100\n",
    "        epoch_loss  += loss.item()\n",
    "\n",
    "        nb_proc = (i+1) * batch_size\n",
    "        if (((i+1) % (nb_batch//2)) == 0):\n",
    "            print(\"  {:2d}  Loss: {:.3f}  Acc: {:2.1f}\".format(nb_proc, epoch_loss/(i+1), epoch_score/(i+1)))\n",
    "    return epoch_loss / nb_batch, epoch_score / nb_batch\n",
    "\n",
    "for i in range(10):\n",
    "    train_one_epoch(net, batch_size, optim, dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed samples\n",
    "embeddings = []\n",
    "net.eval()\n",
    "with torch.autograd.no_grad():\n",
    "    for i, (g_input, g_true) in enumerate(dataloader):\n",
    "        f = g_input.ndata['feat']\n",
    "        hits_emb = net(g_input)\n",
    "        embeddings.append(hits_emb.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  0  0  0  1  0  2  3  4  1  5  5  0  2  6  3 -1  2  6  3  3  4  6  3\n",
      "  3  4  5  5  5  5  5  5  5  5  5  5  1  1  0  1  1  0  1  0 -1  0  2  2\n",
      "  0  2  2  6  3 -1  6  3  4  3  3  3  3  4  4  4  1  4  5  5  3  4  4  7\n",
      "  7  7  7  7  7  7  7  7  7  7  8  8  9  9 -1 10 11 12 13 14 15  8 -1 -1\n",
      " 10 13 12 11  8  9 16 16 10 -1 13 11 12 12  8 16  9 10 13 12 -1 14 14 15\n",
      " 15 17 17 17 14 15 15 17 14 15 15 17 14 15 15 17 15 15 17 17 17  8  8  8\n",
      "  9 16 16 10 13 12 12 -1  8 16  9 16 13 10 12 -1  9  9 16 13 10 12 16  9\n",
      " 13 10 11 14 11 11 14 15 14 15 15 14 15 14 15 17 16  9 13 10 16  9 13 10\n",
      " 10  1 18 18 18 18 18  1 18 18 18 18 18 19 20 21 22 23 23 19 -1 20 21 22\n",
      " 23 19 20 21 22 -1 22 23 24 24 23 23 24 24 23 23 24 24 23 24 24 23 24 24\n",
      " 24 24 18 18 19 19 20 19 20 19 20 19 20 20 22 20 22 22 23 23 23 23 19 19\n",
      " 19 25 25 19 25 26 26 26 26 27 26 26 27 26 26 26 27 26 26 26 27 27 27 27\n",
      " 27 27 28 29 30 31 32 28 28 29 30 31 -1 33 29 29 30 32 32 31 29 30 32 31\n",
      " 27 27 33 27 33  1 33 33 -1 33 29 29 30 32 31 29 30 34 31 29 30 32 34 29\n",
      " 30 -1 -1 31 31 33 33 33 33 30 34 31 31]\n"
     ]
    }
   ],
   "source": [
    "# Cluster\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "c = DBSCAN(eps=.1, min_samples=3)\n",
    "clusters = c.fit_predict(embeddings[0])\n",
    "print(clusters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
